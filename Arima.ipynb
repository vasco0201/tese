{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "dataset = pd.read_csv('CT15Mn-150818_101018/dados_camara.csv')#dados mais recentes\n",
    "#tomtom = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\tomtom_data.csv\")\n",
    "#dataset = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\periodic_data.csv\") #dados periodicos gerados automaticamente\n",
    "#dataset = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\dados_old.csv\") #dados mais antigos\n",
    "dataset['unique_id'] = dataset.Zona.astype(str) + '_' + dataset.ID_Espira.astype(str)\n",
    "dataset['unique_id'] = dataset['unique_id'].str.lower()\n",
    "dataset = dataset[dataset[\"unique_id\"] == \"4_ct4\"]\n",
    "dataset = dataset.drop(columns=[\"unique_id\"])\n",
    "\n",
    "dataset['Data'] = pd.to_datetime(dataset['Data'], infer_datetime_format=True)\n",
    "#print(dataset[\"Data\"])\n",
    "\n",
    "############# SHOULD BE INPUT FROM USER ###############\n",
    "\n",
    "start_date = np.datetime64('2018-08-20')\n",
    "end_date = np.datetime64('2018-08-24')\n",
    "\n",
    "#######################################################\n",
    "\n",
    "test = dataset[(dataset[\"Data\"]>=start_date) & (dataset[\"Data\"]<=end_date)]\n",
    "#print(test.head())\n",
    "test = test.values\n",
    "\n",
    "############## GETTING ALL THE TIME INTERVALS BETWEEN THE SELECTED DATES ########################\n",
    "start_time = datetime.strptime('2018-08-20', '%Y-%m-%d')\n",
    "end_time = datetime.strptime('2018-08-25', '%Y-%m-%d')\n",
    "def datetime_range(start, end, delta):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta\n",
    "hours = [dt.strftime('%Y-%m-%d %H:%M:%S') for dt in datetime_range(start_time,end_time, timedelta(minutes=15))]\n",
    "\n",
    "############# CREATING DATASET ########################\n",
    "def create_data(filename,dataset, hours):    \n",
    "    #o = open(\"arima_sem_data.csv\", \"w\", newline='')\n",
    "    #f = open(\"arima_data.csv\",\"w\", newline='')\n",
    "    f = open(str(filename)+\".csv\",\"w\", newline='')\n",
    "    out_file =  csv.writer(f, delimiter=',',quoting=csv.QUOTE_ALL) #csv.QUOTE_ALL)\n",
    "    out_file.writerow([\"Date/Step\", \"Count\"])\n",
    "\n",
    "    #new_file =  csv.writer(o, delimiter=',',quoting=csv.QUOTE_ALL) #csv.QUOTE_ALL)\n",
    "    #new_file.writerow([\"Date/Step\", \"Count\"])\n",
    "\n",
    "    lazy_hack = 0 \n",
    "    lol = 0\n",
    "    for row in range(len(dataset)):\n",
    "        step = 1\n",
    "        curr_values = dataset[row][4:]\n",
    "        for value in range(len(curr_values)):\n",
    "            out_file.writerow([hours[lazy_hack],curr_values[value]])\n",
    "            lazy_hack+=1\n",
    "            #new_file.writerow([lazy_hack, curr_values[value]])\n",
    "            lol+=1\n",
    "            f.flush()\n",
    "            #o.flush()\n",
    "    f.close()\n",
    "    #o.close()\n",
    "create_data(\"arima_data\", test, hours)\n",
    "#create_data(\"arima_sem_data\", test, hours)\n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "######## TEST SET FIXME!!!! ####################################\n",
    "test_date = np.datetime64('2018-08-27')\n",
    "#end_test_date = np.datetime64('2018-08-28')\n",
    "start_time = datetime.strptime('2018-08-27', '%Y-%m-%d')\n",
    "end_time = datetime.strptime('2018-08-28', '%Y-%m-%d')\n",
    "test_set = dataset[(dataset[\"Data\"]==test_date)] # & (dataset[\"Data\"]<=end_date)]\n",
    "test_set = test_set.values\n",
    "\n",
    "hours_set = [dt.strftime('%Y-%m-%d %H:%M:%S') for dt in datetime_range(start_time,end_time, timedelta(minutes=15))]\n",
    "create_data(\"test_data\", test_set, hours_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as ply\n",
    "import cufflinks as cf\n",
    "########### READING DATASET #####################\n",
    "#arima_data2 = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\arima_sem_data.csv\")#dados mais recentes\n",
    "#print(arima_data.head())\n",
    "def parsers(x):\n",
    "    return datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#arima_data = read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\arima_data.csv\",header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parsers)\n",
    "\n",
    "#print(arima_data.head())\n",
    "#arima_data2.iplot(title=\"Traffic flow over a period of 5 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp        flow\n",
      "0 2004-01-01 00:00:00  201.333333\n",
      "1 2004-01-01 00:15:00  157.333333\n",
      "2 2004-01-01 00:30:00  300.000000\n",
      "3 2004-01-01 00:45:00  356.000000\n",
      "4 2004-01-01 01:00:00  440.000000\n",
      "[[Timestamp('2004-01-01 00:00:00') 201.33333333333334]\n",
      " [Timestamp('2004-01-01 00:15:00') 157.33333333333334]\n",
      " [Timestamp('2004-01-01 00:30:00') 300.0]\n",
      " ...\n",
      " [Timestamp('2004-05-05 23:15:00') 533.3333333333334]\n",
      " [Timestamp('2004-05-05 23:30:00') 482.6666666666667]\n",
      " [Timestamp('2004-05-05 23:45:00') 469.3333333333333]]\n",
      "622\n",
      "12096\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_sas(\"15minute_ByLaneData/md2004_15min_2a.sas7bdat\") #md2004_15min_4b.sas7bdat\n",
    "data_np = dataset.values\n",
    "print(data_np)\n",
    "miss_values = 0\n",
    "for i in range(len(data_np)):\n",
    "    if (math.isnan(data_np[i][1])):\n",
    "        miss_values +=1\n",
    "        past_values = len(data_np[:i])\n",
    "        if past_values >= 4:\n",
    "            temp = data_np[i-4:i]\n",
    "            temp_2 = temp[:,1]\n",
    "            avg = np.mean(temp_2)\n",
    "            data_np[i][1] = avg\n",
    "            #print(\"Changing Nan to :\",avg)\n",
    "        else:\n",
    "            temp = data_np[i-past_values:i][1]\n",
    "            temp_2 = temp[:,1]\n",
    "            avg = np.mean(temp_2)\n",
    "            data_np[i][1] = avg\n",
    "            #print(\"(ELSE) Changing Nan to :\",avg)\n",
    "print(miss_values)\n",
    "print(len(data_np))\n",
    "df_data = pd.DataFrame(data_np)\n",
    "df_data.columns = ['Data', 'Count']\n",
    "\n",
    "df_data.to_csv(\"freeway_data1\" + \".csv\", sep=',', index=False)\n",
    "\n",
    "#dataset.to_csv(\"freeway_data1.csv\",sep = \",\", index=False)\n",
    "#dataset['Data'] = pd.to_datetime(dataset['Data'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8104 3992\n",
      "[[201.33333 157.33333 300.     ]\n",
      " [157.33333 300.      356.     ]\n",
      " [300.      356.      440.     ]\n",
      " ...\n",
      " [714.6667  738.6667  802.6667 ]\n",
      " [738.6667  802.6667  748.     ]\n",
      " [802.6667  748.      770.6667 ]]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('freeway_data1.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "trainX, trainY = create_dataset(train,3)\n",
    "testX, testY =create_dataset(test,3)\n",
    "print(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyramid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-87e9a2127a5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_mpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseasonal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseasonal_decompose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyramid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marima\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauto_arima\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#arima_data[\"Count\"] = pd.to_numeric(arima_data[\"Count\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(arima_data[\"Date/Step\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyramid'"
     ]
    }
   ],
   "source": [
    "########## EXPERIMENTING WITH ARIMA #############\n",
    "from plotly.plotly import plot_mpl\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pyramid.arima import auto_arima\n",
    "#arima_data[\"Count\"] = pd.to_numeric(arima_data[\"Count\"])\n",
    "#print(arima_data[\"Date/Step\"])\n",
    "#x = arima_data.values\n",
    "#print(x[:,0,None])\n",
    "#arima_data.index = x[:,0]\n",
    "#arima_data.index = pd.to_datetime(arima_data.index)\n",
    "#print(arima_data.head())\n",
    "#sys.exit()\n",
    "result = seasonal_decompose(arima_data, model='multiplicative',freq=5)\n",
    "fig = result.plot()\n",
    "fig.show()\n",
    "\n",
    "\n",
    "############### FINDING THE BEST PARAMETERS FOR P,Q,D\n",
    "\n",
    "\n",
    "stepwise_model = auto_arima(data, start_p=1, start_q=1,\n",
    "                           max_p=3, max_q=3, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "print(stepwise_model.aic())\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "model = ARIMA(arima_data, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
