{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "curr_dir = os.getcwd()\n",
    "data_folder = os.path.join(curr_dir,\"dados_camara_todos.csv\")\n",
    "dataset = pd.read_csv(data_folder)\n",
    "#dataset = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\CT15Mn-150818_101018\\\\dados_camara.csv\")#dados mais recentes\n",
    "#tomtom = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\tomtom_data.csv\")\n",
    "#dataset = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\periodic_data.csv\") #dados periodicos gerados automaticamente\n",
    "#dataset = pd.read_csv(\"\\\\Users\\\\ASUS\\\\Documents\\\\IST\\\\5ºAno\\\\dados_old.csv\") #dados mais antigos\n",
    "dataset['unique_id'] = dataset.Zona.astype(str) + '_' + dataset.ID_Espira.astype(str)\n",
    "dataset['unique_id'] = dataset['unique_id'].str.lower()\n",
    "dataset_uid = dataset[(dataset[\"unique_id\"] == \"4_ct4\")]\n",
    "\n",
    "dataset_uid = dataset_uid.drop(columns=[\"Zona\",\"Contadores\",\"ID_Espira\",\"unique_id\"])\n",
    "dt2 = copy.deepcopy(dataset_uid)\n",
    "#dataset.sort_values(['Data'],ascending=True).groupby('Data').reset_index()\n",
    "\n",
    "dataset_uid = dataset_uid.groupby('Data').apply(lambda x: x.reset_index())\n",
    "\n",
    "msk = np.random.rand(len(dataset_uid)) < 0.7\n",
    "train_df = dataset_uid[msk]\n",
    "test_df = dataset_uid[~msk]\n",
    "\n",
    "dt2.to_csv(\"lstm/limited_data.csv\", sep= ',', index=False)\n",
    "#train_set\n",
    "train_df = train_df.drop(columns=[\"index\"])\n",
    "\n",
    "train_df.to_csv(\"lstm/train.csv\", sep= ',', index=False)\n",
    "\n",
    "#test_set\n",
    "test_df = test_df.drop(columns=[\"index\"])\n",
    "test_df.to_csv(\"lstm/test.csv\", sep= ',', index=False)\n",
    "\n",
    "#full set\n",
    "dataset_uid = dataset_uid.drop(columns=[\"index\"])\n",
    "dataset_uid.to_csv(\"lstm/dados_nn.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464\n",
      "Number of changes: 3205\n",
      "640\n",
      "Number of changes: 1370\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import statistics\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_cp = copy.deepcopy(train_df)\n",
    "test_cp = copy.deepcopy(test_df)\n",
    "\n",
    "def smooth_data(train_cp,filename):\n",
    "    data = train_cp.iloc[:,0]\n",
    "    print(len(data.values))\n",
    "    train_cp = train_cp.drop(columns=[\"Data\"])\n",
    "    train_cp = train_cp.values\n",
    "\n",
    "    #train_cp = train_cp.astype('float32')\n",
    "\n",
    "    avg_list = []\n",
    "    std_list = []\n",
    "    for i in range(len(train_cp[0])):\n",
    "        curr_avg = sum(train_cp[:,i])/len(train_cp[:,i])\n",
    "        avg_list.append(curr_avg)\n",
    "        std_list.append(statistics.stdev(train_cp[:,i]))\n",
    "    #FIX sera que e preciso guardar os valores antigos e usá-los para fazer os updates? \n",
    "    #agora esta a fazer uma especie de moving average, utilizando os valores novos nos updates seguintes\n",
    "\n",
    "\n",
    "    number_of_changes= 0\n",
    "    for row in range(len(train_cp)):\n",
    "        \n",
    "        for column in range(len(train_cp[0])):\n",
    "            if (train_cp[row][column] > (avg_list[column] + 2*std_list[column])):\n",
    "                old_value = train_cp[row][column]\n",
    "                if column == 0:\n",
    "                    previous_t = train_cp[row,-1]\n",
    "                    next_t = train_cp[row,column+1]\n",
    "                    train_cp[row,column] = (previous_t + next_t)/2\n",
    "                    number_of_changes+=1\n",
    "                if column == 95:\n",
    "                    previous_t = train_cp[row,column-1]\n",
    "                    next_t = train_cp[row,0]\n",
    "                    train_cp[row,column] = (previous_t + next_t)/2\n",
    "                    number_of_changes+=1\n",
    "                else:\n",
    "                    previous_t = train_cp[row,column-1]\n",
    "                    next_t = train_cp[row,column+1]\n",
    "                    train_cp[row,column] = (previous_t + next_t)/2\n",
    "                    number_of_changes+=1\n",
    "                new_value = train_cp[row][column]\n",
    "    print(\"Number of changes: \" + str(number_of_changes))\n",
    "                \n",
    "    new_d = data.values.reshape(len(data.values),1)\n",
    "    test = np.concatenate((new_d,train_cp),axis=1)\n",
    "    pd.DataFrame(test).to_csv(\"lstm/\" + str(filename) + \".csv\", sep=',', index=False)\n",
    "    return test, train_cp\n",
    "smooth_train, df_train= smooth_data(train_cp, \"train_2\")\n",
    "smooth_test, df_test = smooth_data(test_cp, \"test_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(in_file, out_file, nrows=-1):\n",
    "    in_file = open(str(in_file),\"r\")\n",
    "    next(in_file)\n",
    "    out_file = open(str(out_file),\"w\")\n",
    "    out_file.write(\"t-3,t-2,t-1,Y\\n\") #header\n",
    "    k = 0    \n",
    "    lines = in_file.readlines()\n",
    "    for line in lines:\n",
    "        line = line.split(\",\")\n",
    "        line = line[1:]\n",
    "        line[-1] = line[-1].replace(\"\\n\",\"\") #last data record has a \\n\n",
    "        its = [iter(line), iter(line[1:]), iter(line[2:]),iter(line[3:])] #Construct the pattern for longer windowss\n",
    "        x = list(zip(*its))\n",
    "        if (k == nrows):\n",
    "            break\n",
    "            \n",
    "        k+=1 \n",
    "#print(x)\n",
    "    #j = 0\n",
    "    #while(j<50): #this cycle was for creating a mock dataset with repeated data for testing purposes\n",
    "        for i in x:\n",
    "        #print(i[0],i[1],i[2],i[3])\n",
    "            out_file.write(i[0] + \",\" + i[1] +\",\" + i[2] +\",\"+ i[3])\n",
    "            out_file.write(\"\\n\")\n",
    "     #   j+=1\n",
    "    \n",
    "    in_file.close()\n",
    "    out_file.close()\n",
    "transform_data(\"lstm/train_2.csv\",\"lstm/train_formatted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136152\n",
      "[[19. 17. 15.]\n",
      " [17. 15. 10.]\n",
      " [15. 10.  9.]\n",
      " ...\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[10.  9. 18. ...  0.  0.  0.]\n",
      "Epoch 1/200\n",
      "136152/136152 [==============================] - 81s 596us/step - loss: 67.3682\n",
      "Epoch 2/200\n",
      "136152/136152 [==============================] - 75s 554us/step - loss: 55.4721\n",
      "Epoch 3/200\n",
      "136152/136152 [==============================] - 80s 587us/step - loss: 55.0271\n",
      "Epoch 4/200\n",
      " 26784/136152 [====>.........................] - ETA: 1:11 - loss: 54.2250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ff69bc9496f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tese/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tese/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tese/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tese/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "import sys\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv('lstm/train_formatted.csv')\n",
    "dataset = dataframe.values\n",
    "train_set = dataset.astype('float32')\n",
    "\n",
    "\n",
    "def create_dataset(data):\n",
    "    X,Y = [],[]\n",
    "    for i in data:\n",
    "        X.append(i[:3])\n",
    "        Y.append(i[3])\n",
    "    return numpy.array(X), numpy.array(Y)\n",
    "print(len(train_set))\n",
    "trainX, trainY = create_dataset(train_set)\n",
    "print(trainX)\n",
    "print(trainY)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "trainX = trainX.reshape((trainX.shape[0], trainX.shape[1], n_features))\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "# fit model\n",
    "model.fit(trainX, trainY, epochs=200, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tese] *",
   "language": "python",
   "name": "conda-env-tese-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
